# Algorithm_2

# `04.05`

## 트리

- 비선형 구조
- 원소들 간에 1:n 관계를 가지는 자료구조
- 원소들 간에 계층관계를 가지는 계층형 자료구조
- 상위 원소에서 하위 원소로 내려가면서 확장되는 트리모양의 구조

### 정의

> - 한 개 이상의 노드로 이루어진 유한 집합이며 다음 조건을 만족한다.
>   - 노드 중 최상위 노드를 루트(root)라 한다.
>   - 나머지 노드들은 n(>=0)개의 분리 집합 T1, ..., TN으로 분리될 수 있다.
> - 이들 T1, ..., TN은 각각 하나의 트리가 되며(재귀적 정의) 루트의 부 트리(subtree)라 한다.
> - 단말노드(리프노드(leaf node)) : 마지막에 있는 노드(자식이 없는 노드)

### 용어

> - 노드(node) - 트리의 원소
> - 간선(edge) - 노드를 연결하는 선, 부모 노드와 자식 노드를 연결
> - 루트 노드(root node) - 트리의 시작 노드(부모가 없는 노드)
>
> - 형제 노드(sibling node) - 같은 부모 노드의 자식 노드들
> - 조상 노드 - 간선을 따라 루트 노드까지 이르는 경로에 있는 모든 노드들
> - 서브 트리(subtree) - 부모 노드와 연결된 간선을 끊었을 때 생성되는 트리
> - 자손 노드 - 서브 트리에 있는 하위 레벨의 노드들
> - 차수(degree)
>   - 노드의 차수 : 노드에 연결된 자식 노드의 수
>   - 트리의 차수 : 트리에 있는 노드의 차수 중에서  가장 큰 값(연결된 노드의 수 중 가장 큰 값)
>   - 단말 노드(리프 노드) : 차수가 0인 노드, 자식 노드가 없는 노드
> - 높이(Level)
>   - 노드의 높이 : 루트에서 노드에 이르는 간선의 수, 노드의 레벨
>     - 책마다 시작값(0, 1)이 다름, 일반적으로 0
>   - 트리의 높이 : 트리에 있는 노드의 높이 중에서 가장 큰 값, 최대 레벨

<br>

## 이진 트리

- 모든 노드들이 2개의 서브 트리를 갖는 특별한 형태의 트리

- 각 노드가 자식 노드를 최대한 2개 까지만 가질 수 있는 트리

  - 왼쪽 자식 노드(left child node)
  - 오른쪽 자식 노드(right child node)

- **자식노드의 수가 무조건 2개가 아니라 없거나 1개일 수도 있다!**

- 모양만 보고 이진트리인지 알 수는 없다. 1:N 관계로 들어왔을 수도 있기 때문에, 이진트리 조건에 만족한다 정도(자식이 2개 이하)

- 레벨 i에서의 노드의 최대 개수는 2^i개

- 높이가 h인 이진 트리가 가질 수 있는 노드의 최소 개수는 (h+1)개가 되며, 최대 개수는 (2^(h+1) -1)개가 된다.

  - ex) h=3 이면 최대개수 = 2^3 + 2^2 + 2^1 + 2^0

  - 등비수열이나 이진수로 구할 수 있다!

### 포화 이진 트리(Full Binary Tree)

> - 모든 레벨에 노드가 포화 상태로 차 있는 이진 트리(전부다 2개씩)
> - 높이가 h일 때, 최대의 노드 개수가 (2^(h+1) -1)의 노드를 가진 이진 트리
> - 루트를 1번으로 하여 2^(h+1) -1까지 정해진 위치에 대한 노드 번호를 가짐
> - 따로 취급하기 위해서 이름을 붙인 것!

### 완전 이진 트리(Complete Binary Tree)

> - 높이가 h이고 노드 수가 n개일 때 (단, h+1 <= n <= 2^(h+1) -1), 포화 이진 트리의 노드 번호 1번부터 n번까지 빈 자리가 없는 이진 트리
> - 마지막 레벨 까지 꽉 차있을 수 도 있고 아닐 수도 있음, 대신 중간에 비어있으면 안됨

### 편향 이진 트리(Skewed Binary Tree)

> - 높이 h에 대한 최소 개수의 노드를 가지면서 한쪽 방향의 자식 노드만을 가진 이진 트리
>   - 왼쪽 편향 이진 트리
>   - 오른쪽 편향 이진 트리
> - 비선형 자료의 장점이 없어진 모양 --> 선형 모양
>   - 리스트를 앞에서부터 찾는 것과 같음

### 순회(traversal)

> - 트리의 각 노드를 중복되지 않게 전부 방문(visit) 하는 것을 말하는데 트리는 비 선형 구조이기 때문에 선형구조에서와 같이 선후 연결 관계를 알 수 없다. 따라서 특별한 방법이 필요!
> - 순회 : 트리의 노드들을 체계적으로 방문하는 것
> - 3가지의 기본적인 순회방법
>   - 전위순회(preorder traversal) : VLR
>     - 부모 노드 방문 후, 자식 노드를 좌, 우 순서로 방문
>   - 중위순회(inorder traversal) : LVR
>     - 왼쪽 자식노드, 부모노드, 오른쪽 자식노드 순으로 방문
>     - 부모노드부터 방문하지만 처리하지는 않고 왼쪽 자식노드로 가서 먼저 처리
>   - 후위순회(postorder traversal) : LRV
>     - 자식노드를 좌우 순서로 방문한 후, 부모노드로 방문
> - 전위와 후위를 같이 사용도 가능!
> - 순회하면서 꺼내는 것만이 아니라 집어넣을 수도 있음
> - 손으로 따라갈 수 있어야함!
> - DFS로 할 수 있지만 재귀로 구현함(구분하는 목적이 있기 때문에)
>
> #### 1. 전위 순회(preorder traversal)
>
> > - 수행 방법
> >   1. 현재 노드 n을 방문하여 처리 - V
> >   2. 현재 노드 n의 왼쪽 서브트리로 이동 - L
> >   3. 현재 노드의 n의 오른쪽 서브트리로 이동 - R
>
> #### 2. 중위 순회(inorder traversal)
>
> > - 수행 방법
> >   1. 현재 노드 n의 왼쪽 서브트리로 이동 - L
> >   2. 현재 노드 n을 방문하여 처리 - V
> >   3. 현재 노드의 n의 오른쪽 서브트리로 이동 - R
>
> #### 3. 후위 순회(postorder traversal)
>
> > - 수행 방법
> >   1. 현재 노드 n의 왼쪽 서브트리로 이동 - L
> >   2. 현재 노드의 n의 오른쪽 서브트리로 이동 - R
> >   3. 현재 노드 n을 방문하여 처리 - V

### 표현(어떤 이진 트리냐에 따라 다름)

> - 배열을 이용한 이진 트리의 표현
>   - 이진 트리에 각 노드 번호를 부여
>   - 루트의 번호를 1로 함
>   - 레벨 n에 있는 노드에 대하여 왼쪽부터 오른쪽으로 2^n 부터 2^(n+1) -1 까지 번호를 차례로 부여
> - 노드의 성질(포화 이진 트리)
>   - 노드 번호가 i인 노드의 부모 노드 번호 : l_i/2_l (floor 연산 i/2를 넘지 않는 가장 큰 값)
>     - int(-3/2) = -1, -3//2 = -2(floor연산)
>   - 노드 번호가 i인 노드의 왼쪽 자식 노드 번호 : 2*i
>   - 노드 번호가 i인 노드의 오른쪽 자식 노드 번호 : 2*i + 1
>   - 레벨 n의 노드 번호 시작 번호 : 2^n
> - 노드 번호를 배열의 인덱스로 사용
> - 배열을 이용한 이진 트리 표현의 단점
>   - 편향 이진 트리의 경우에 사용하지 않는 배열 원소에 대한 메모리 공간 낭비가 발생
>   - 트리의 중간에 새로운 노드를 삽입하거나 기존의 노드를 삭제할 경우 배열의 크기 변경이 어려워 비효율적
> - 단점을 보완하기 위해 연결리스트를 이용할 수 있다.
> - 주의!!(명시 되어있는지 확인!!)
>   - 포화 이진 트리인지 여부
>   - 루트가 1번이다 X
>   - 부모번호가 더 작은 경우가 많지만 무조건은 아님, 더 클 수가 있음

<br>

## 수식 트리

- 수식을 표현하는 이진 트리
- 수식 이진 트리(Expression Binary Tree)라고 부르기도 함
- 연산자는 루트 노드이거나 가지 노드
- 피연산자는 모두 잎 노드

> - 중위 순회 : A / B * C * D + E
> - 후위 순회 : A B / C * D * E +
> - 전위 순회 : + * * / A B C D E

<br>

## 이진 탐색 트리

- 탐색 작업을 효율적으로 하기 위한 자료 구조
- 모든 원소는 서로 다른 유일한 키를 갖는다.
- key(왼쪽 서브트리) < key(루트 노드) < key(오른쪽 서브트리)
- 왼쪽 서브트리와 오른쪽 서브트리도 이진 탐색 트리다.
- 중위 순회하면 오름차순으로 정렬된 값을 얻을 수 있다.

### 탐색 연산

> - 루트에서 시작한다.
> - 탐색할 키 값x를 루트 노드의 키 값과 비교한다.
>   - (키 값 x = 루트노드의 키 값)인 경우 : 원하는 원소를 찾았으므로 탐색연산 성공
>   - (키 값 x < 루트노드의 키 값)인 경우 : 루트노드의 왼쪽 서브트리에 대해서 탐색연산
>   - (키 값 x > 루트노드의 키 값)인 경우 : 루트노드의 오른쪽쪽 서브트리에 대해서 탐색연산
> - 순환하면서 탐색 연산
> - 이진 탐색 트리(좌우가 잘 맞춰진) 장점 : 선형으로 탐색하는 것보다 빠름, 탐색 깊이를 주일 수 있음 → 탐색 시간을 줄임

### 삽입 연산

> - 먼저 탐색 연산을 수행
>   - 삽입할 원소와 같은 원소가 트리에 있으면 삽입할 수 없으므로, 같은 원소가 트리에 있는지 탐색하여 확인
>   - 탐색에서 탐색 실패가 결정되는 위치가 삽입 위치
> - 탐색 실패한 위치에 원소를 삽입

### 성능

> - 탐색(searching), 삽입(insertion), 삭제(deletion) 시간은 트리의 높이 만큼 시간이 걸림
>   - O(h), h: BST의 깊이(height)
> - 평균의 경우
>   - 이진 트리가 균형적으로 생성되어 있는 경우
>   - O(log n)
> - 최악의 경우
>   - 한쪽으로 치우친 경사 이진트리의 경우
>   - O(n)
>   - 순차탐색과 시간복잡도가 같음
> - 얼마나 좌우균형이 맞춰서 삽입이 이루어지느냐에 따라 성능이 달라짐
> - 검색 알고리즘의 비교
>   - 배열에서의 순차 검색 : O(N)
>   - 정렬된 배열에서의 순차 검색 : O(N)
>   - 정렬된 배열에서의 이진 탐색 : O(logN)
>     - 고정 배열 크기와 삽입, 삭제 시 추가 연산 필요
>   - 이진 탐색트리에서의 평균 : O(logN)
>     - 최악의 경우 : O(N)
>     - 완전 이진 트리 또는 균형트리로 바꿀 수 있다면 최악의 경우를 없앨 수 있음
>       - 새로운 원소를 삽입할 때 삽입 시간을 줄인다.
>       - 평균과 최악의 시간이 같다. O(logN) → 레드블랙트리
> - 해쉬 검색 : O(1)
>   - 추가 저장 공간이 필요

<br>

## 힙(heap)

- 완전 이진 트리에 있는 노드 중에서 키값이 가장 큰 노드나 키값이 가장 작은 노드를 찾기 위해서 만든 자료구조
- 왼쪽과 오른쪽 자식의 대소 관계는 중요하지 않고 부모 자식 간의 대소 관계를 따진다.
- 최대 힙(max heap)
  - 키값이 가장 큰 노드를 찾기 위한 **완전 이진 트리**
  - 부모노드의 키값 > 자식노드의 키값
  - 루트 노드 : 키값이 가장 큰 노드
- 최소 힙(min heap)
  - 킥값이 가장 작은 노드를 찾기 위한 **완전 이진 트리**
  - 부모노드의 키값 < 자식노드의 키값
  - 루트 노드 : 키값이 가장 작은 노드
- 힙의 키를 우선순위로 활용하여 우선순위 큐를 구현할 수 있다.

### 삽입

> - 마지막 노드 + 1에 삽입
> - 삽입 후 부모와 비교하여 이동(p = c // 2)

### 삭제

> - 루트 노드의 원소만을 삭제할 수 있다.
> - 루트 노드의 원소를 삭제하여 반환한다.
> - 힙의 종류에 따라 최대값 또는 최솟값을 구할 수 있다.
> - 마지막 노드를 삭제해야함(last를 감소)
> - 순서
>   - 루트 노드의 원소 삭제 - 마지막 노드 삭제 - 삽입노드 < 자식노드이면 자리 바꾸기 - 자리 확정

<br>

## :memo: 

>### 트리
>
>- 루트로부터 그래프를 탐색하는 것
>- 데이터를 의미 있는 구조로 저장할 때 트리를 자주 사용함(폴더 구조)
>- 자연스럽게 그룹으로 나누어짐
>- 이진 트리는 나중에 이진 탐색에 활용함
>- 1차 배열 2개를 쓰는 방법 / 2차 배열  한 개를 쓰는 방법이 있음 → 2개만 있기 때문에
>- 이진 트리를 저장할 때
>  - 각 노드에 대해서 왼쪽 자식이 있는지 없는지, 왼쪽 자식이 누구인지, 부모가 누구인지
>  - → 자식 정보와 부모 정보를 저장
>- 트리를 순회할 때는 방문 표시를 안해도 된다!
>  - 한 방향으로만 순회하기 때문에(부모 → 자식)
>  - 되돌아가지 않아도 된다. + 사이클이 없음
>  - 대신 마지막에 왔는지 안왔는지를 판단해야함(더이상 갈 곳이 있는지)
>    1. 지금 보고있는 노드가 단말노드인지 판단
>    2. 특별한 노드를 하나씩 달아놓아서 판단(공백 노드(NULL node))
>       - 왼쪽과 오른쪽 자식이 없다면 없다라는 표시
>- 위치를 정해두고 없으면 비워두는 형식으로 저장한다.
>- Heap : 완전이진트리 형태의 구조를 저장함 - 스택과 비슷한 개념으로 FILO, 1차 배열 형태로 저장 → 정확하지 않음
>- 노드 수가 V 라면 간선 수는 V-1 → 이렇게 해야 사이클이 없음
>
>### 이진 트리
>
>- 이진트리 순회 : 모든 노드를 3번 거쳐간다.
>  - 처음 노드에 진입할 때
>  - 왼쪽 자식에서 돌아올 때
>  - 오른쪽 자식에서 돌아올 때
>- DFS 방식 - 왼쪽 자식 먼저 가본다.
>
>### 참고
>
>- C나 java에서는 입력이 각 줄마다 몇개씩 들어오는지 알려줘야한다!
>- 완전이진트리와 같은 문제는 노드의 개수로 입력의 개수로 알 수 있다.

<br>

# `04.06`

## 최소공통조상(LCA)

- 트리에서 기본적으로 풀어야하는 유명한 문제!
- 트리 : 문자열 처리에서 많이 활용됨
  - 패턴 매칭, 압축

<br>

## 이진탐색트리

- 왜 사용하는가? 배열에 저장에서 쓰거나, 이진트리를 쓰면 되지 않나?

  - 배열에서 정렬을 하고 찾을 때 O(logN)
  - 이진탐색트리에서 찾을 때 O(logN)

  → `계속 정렬된 상태를 유지하기 위해서 사용`

  - 찾을 때는 O(logN)으로 같지만 삽입하고 삭제할 때 다름
  - 배열에 삽입 삭제할 경우는 한칸씩 앞으로 밀거나 뒤로 미는 작업을 추가적으로 해야 함, 최악의 경우에는 O(N)이 걸림
  - `이진탐색트리는 삽입하거나 삭제할 때 O(logN)이 걸림`
  - 일반 상황에서 자료의 배치는 빈번하게 변하는데 그 와중에 탐색을 할 때 용이

- 어떤 노드를 루트로 사용하더라도 높이 차이가 1이내로 들어오도록 해야 함

- 단말노드까지 가도 못찾는 경우 : 탐색 실패(최악의 경우 트리의 높이 만큼의 경우의 수가 필요함) → 높이를 logN으로 만듦

<br>

## 해쉬 검색

- O(1) : 상수 시간 → 자료가 많던 적던 찾는 시간이 일정, 저장 공간이 필요(메모리)
- 메모리 사용량을 늘리고 시간을 줄인다.
- 배열에서 index를 알고 바로 찾아가는 경우 O(1)
- python의 Dictionary가 해싱으로 구성되어 있다.
- 추가적인 단점
  - 저장된 순서를 알 수 없음
  - 자료가 있는지 없는지 탐색은 빠르지만 읽어오는 순서는 알 수 없음

### 직접 번지 테이블 방식

> - ex) 회사의 사번(unique key value)을 통해서 값을 찾음, 사번에 해당하는 메모리 공간 확보 필요
> - 키 값을 배열의 index로 하여 원하는 값을 바로 찾음(해싱)
> - 직접 매핑은 메모리를 너무 많이 사용(ex. naver의 회원가입 때 정보를 저장할 메모리 - 너무 큼)
> - Key값에 해당하는 값을 mapping시켜줌 : 해시함수가 매핑시켜주는 역할을 한다!

### 간접 번지 테이블 방식

> - 해시함수를 통해 변환시켜서 찾아가는 것
> - 계산에 의해서 바꿔서 나오는데 우연히 그 값이 같게 나와서 충돌이 일어날 수 있다. 그에 따른 해결방법이 있다.
>   - 해결 방법은 나중에 배우는 걸로..

<br>

`04.08`

## 병합 정렬(Merge Sort)

- 여러 개의 정렬된 자료의 집합을 병합하여 한 개의 정렬된 집합으로 만드는 방식

  - 두 개를 합쳐서 하나로 만듦
  - n/2

- 분할 정복 알고리즘 활용

  - 자료를 최소 단위의 문제까지 나눈 후에 차례대로 정렬하여 최종 결과를 얻어냄
  - top-down 방식

- 시간 복잡도 : O(nlogn)

- 2개의 부분 집합을 정렬하면서 하나의 집합으로 병함

  - n개의 부분 집합이 1개로 병합될 때까지 반복함

- Python 스타일

  ```python
  def mergeSort(lst):
      if len(lst) <= 1: return lst  # 길이가 1이면 더이상 분할해 줄 필요가 없음
  
      # 분할 하기
      mid = len(lst) // 2
      l = mergeSort(lst[:mid])
      r = mergeSort(lst[mid:])  # 이렇게 하면 안좋지만 개념을 이해하려고 씀
  
      # 병합 하기
      result = []  # 하나씩 비교해서 작은 것을 집어 넣을거임!
      while len(l) and len(r):
          if l[0] < r[0]:
              result.append(l.pop(0))
          else:
              result.append(r.pop(0))
  
      if len(l): result.extend(l)
      else: result.extend(r)
  
      return result
  
  
  
  arr = [69, 10, 30, 2, 16, 8, 31, 22]
  
  sorted_lst = mergeSort(arr)
  print(sorted_lst)
  ```

- C 스타일

  ```python
  arr = [69, 10, 30, 2, 16, 8, 31, 22]
  tmp = [0] * len(arr) # 원본 크기와 같은 공간을 만듬
  
  def mergeSort(s, e):
      if s == e: return  # 범위에 해당하는 자료가 한개밖에 없을 때
  
      # 분할 하기
      mid = (s + e) // 2
      mergeSort(s, mid)
      mergeSort(mid + 1, e)  # 이렇게 하면 안좋지만 개념을 이해하려고 씀
  
      # 병합 하기
      i, j, k = s, mid + 1, s  # 임시변수를 설정 / s, e 값이 하는 도중에 변하면 안되니까 / k는 tmp의 인덱스 변수
  
      while i <= mid and j <= e:
          if arr[i] < arr[j]:
              tmp[k] = arr[i]
              i, k = i + 1, k + 1
          else:
              tmp[k] = arr[j]
              j, k = j + 1, k + 1
  
      while i <= mid:
          tmp[k] = arr[i]
          i, k = i + 1, k + 1
      while j <= e:
          tmp[k] = arr[i]
          j, k = j + 1, k + 1
  
      for i in range(s, e + 1):
          arr[i] = tmp[i]
          # 지금은 전역변수로 잡았는데 원래는 내부에서 만들어서 사용 / C언어 같은 경우는 배열도 같이 함수로 넘겨주고 마지막에 정렬된 배열을 돌려줌
  
  
  mergeSort(0, len(arr) - 1)
  print(arr)
  ```

<br>

# `04.12`

## 복잡도 분석

### 알고리즘

> - 유한한 단계를 통해 문제를 해결하기 위한 절차나 방법

### 효율

> - 공간적 효율성: 연산량 대비 얼마나 적은 메모리 공간을 요하는 가
> - 시간적 효율성 : 연산량 대비 얼마나 적은 시간을 요하는 가
> - 효율성을 뒤집어 표현하면 복잡도(Complexity)가 된다. 복잡도가 높을수록 효율성은 저하된다.

### 표기

> - O(Big-Oh)-표기
>   - 복잡도의 점근적 상한을 나타낸다.
>   - 실행시간이 O(_)에 비례
> - Ω(Big-Omega)-표기
>   - 복잡도의 점근적 하한을 의미한다.
>   - 최소한 걸리는 시간
> - Θ(Theta)-표기
>   - Big-Oh와 Big-Omega가 같은 경우에 사용
>   - n이 증가함에 따라 Θ(_)과 동일한 증가율을 가진다.
>   - Big-Oh로 간주하고 풀어도 된다.

<br>

## 비트 연산

- 실행 속도가 빠르다.(실행 사이클이 짧다.)
- CPU를 적게 쓴다.
- 때에 따라 메모리 사용량을 줄일 수 있다.
- 수식을 간결하게 표현 가능
- 가독성이 떨어진다.

### 1 << n

> - 2^n의 값을 갖는다.
> - 원소가  n개일 경우의 모든 부분 집합의 수를 의미한다.
> - Power set(모든 부분 집합)
>   - 공집합과 자기 자신을 포함한 모든 부분 집합
>   - 각 원소가 포함되거나 포함되지 않는 경우의 수를 계산

### i & (1 << j)

> - i의 j번째 비트가 1인지 아닌지를 의미
> - 검사할 때 사용
> - j번째 비트가 0인 경우 : 0, 1인 경우 : 0이 아니다.

<br>

## 음수 표현

- 2의 보수로 표현
  - 1의 보수에서 최하위 비트에 1을 더함
  - ex. 1001 → 0110(1의 보수) + 1 → 0111(2의 보수)

<br>

## 진수

- 10진수 → 타 진수로 변환
  - 원하는 타 진법의 수로 나눈 뒤 나머지를 거꾸로 읽는다.
- 타 진수 → 10진수로 변환
  - ex. (135) 8진수 = 1*8^2 + 3*8^1 + 5*8^0
  - ex. (135.12) 8진수 = 1*8^2 + 3*8^1 + 5*8^0 + 1*8^-1 + 2*8^-2 = 93.15625

<br>

## 실수의 표현

- 컴퓨터는 실수를 표현하기 위해 부동 소수점(floating-point) 표기법을 사용
- 부동 소수점 표기 방법은 소수점의 위치를 고정시켜 표현하는 방식
  - 소수점의 위치를 왼쪽의 가장 유효한 숫자 다음으로 고정시키고 밑수의 지수승으로 표현
  - ex. 1001.0011 → 1.0010011 x 2^3

### 실수를 저장하기 위한 형식

> - 단정도 실수(32비트)
>   - 부호 1비트 | 지수 8비트 | 가수 23비트
> - 배정도 실수(64비트)
>   - 부호 1비트 | 지수 11비트 | 가수 52비트
> - 가수부(mantissa) : 실수의 유효 자릿수들을 부호화ㅏ된 고정 소수점으로 표현한 것
>   - ex. 1001.0011 → 1.0010011 x 2^3의 0010011
> - 지수부(exponent) : 실제 소수점의 위치를 지수 승으로 표현한 것
>   - ex. 1001.0011 → 1.0010011 x 2^3의 ^3

### 실수 비교

> - 컴퓨터는 실수를 근사적으로 표현한다
> - 따라서 비교할때 연산하는 과정이 다르면 유효자리에 따라 달라질 수 있음
>   - X == Y가 아니라 | X - Y | ≤ 10^-9 이런식으로 비교한다.
>   - (파이썬은 예외, 내부적으로 더 많은 비트를 사용하여 넓은 범위의 실수를 표현)
>   - 32비트 실수형 유효자릿수(십진수) → 6자리
>   - 64비트 실수형 유효자릿수(십진수) → 15자리

<br>

## 참고

- 기계어로 바꿔서 처리하는 경우는 사이즈(길이)를 정해야 함
- 16진수 한자리는 4bit
- 32bit : 4바이트 타입(기본 단위)으로 데이터를 처리
- LSB, MSB : 제일 낮은 bit과 제일 높은 자리의 bit
- ~ : 인버트 라고 읽음
- 0.625 → int로 받아올 수 없음
- 시간 복잡도
  - 순차 탐색 : O(n)
  - 이진 탐색 : O(logn)
  - 병합 정렬 : O(nlogn)

<br>

## :memo: 통신

> - 프로그램 사이에서 데이터를 주고 받는 것
> - 하드에 있던게 메모리로 올라가는 것 : process
>   - 메모장을 여러개를 띄움 : process를 여러개를 띄움
>   - 여러개의 process를 띄우는 것은 프로그램마다 다름(1개만 가능인 것도 있음)
>   - CPU는 각 process의 루틴을 실행시킬 때 필요 - 왔다갔다하면서 여러 process에서 일을 하는 거임(동시가 아니라 짧은 시간에 움직임)
>     - 옛날에는 1core, 요즘은 core의 개수가 늘어남
>     - core가 여러개가 있으면 동시에 가능함!
> - 우리는 자신의 컴퓨터의 process끼리 통신할 수 있고 다른 컴퓨터와 통신(데이터를 주고 받음)할 수도 있음, 중간에 서버가 있을 수도 있음
> - socket(데이터 채널이 같은 것) 다른 컴퓨터와 통신할 때 많이 쓰임
> - 다른 컴퓨터에 있는 process에 데이터를 보내려면
>   - 찾아가기 위해 ip 주소(하드웨어)가 필요 - 더 복잡하지만 이정도만,,
>   - 컴퓨터 안에 많은 process가 떠있는데 어느 곳으로 가야 하는지 알려줘야 함 → port
>     - port : process를 식별하기 위한 값 !
> - wellknown service : 잘 알려져있어서 port값을 고정해놓음
>   - ex. 웹서버 : 80(HTTP) → 80으로 적혀져있는 데이터를 가져감
>     - 주소창에 치면 80port로 전송됨(default)
>     - 주소창 뒤에 :80

<br>

# `04.14`

## 반복

### 반복 구조

> - 초기화
>   - 반복되는 명령문을 실행하기 전에 (한번만) 조건 검사에 사용할 변수의 초기값 설정
> - 조건검사
> - 반복할 명령문 실행
> - 업데이트
>   - 무한루프가 되지 않게 조건이 거짓이 되게 한다.

### 반복을 이용한 선택 정렬

> - ```python
>   def selectionSort(A):
>   	n = len(A)
>   	for i in range(0, n-1):
>   		min = i
>   		for j in range(i + 1, n):
>   			if A[j] < A[min]
>   				min = j
>   		A[min], A[i] = A[i], A[min]
>   ```

<br>

## 재귀

### 재귀적 알고리즘

> - 하나 또는 그 이상의 기본 경우(base case)
>   - 집합에 포함되어 있는 원소로 induction을 생성하기 위한 시드(seed)역할
> - 하나 또는 그 이상의 유도된 경우(inductive case)
>   - 새로운 집합의 원소를 생성하기 위해 결합 되어지는 방법

### 재귀 함수(Recursive function)

> - 함수 내부에서 직접 혹은 간접적으로 자기 자신을 호출하는 함수
> - 일반적으로 재귀적 정의를 이용해서 구현하고 기본 부분과 유도 파트로 구성된다.
> - 재귀로 작성한느 것은 반복 구조에 비해 간결하고 이해하기 쉽다.
> - 함수 호출은 프로그램 메모리 구조에서 스택을 사용한다. 따라서 재귀호출은 반복적인 스택의 사용을 의미하며 메모리 및 속도에서 성능저하가 발생한다.
> - 스택 안에 다음 재귀의 정보 뿐만 아니라 끝나고 어디로 돌아갈지 등 생각보다 많은 정보를 저장하기 때문에 스택이 부족한 경우가 나타날 수 있다.

<br>

## 반복 vs 재귀

- 해결할 문제를 고려하여 선택
- 재귀는 문제 해결을 위한 알고리즘 설계가 간단하고 자연스럽다.
  
  - 추상 자료형(list, tree 등)의 알고리즘은 재귀적 구현이 간단하고 자연스러운 경우가 많다.
- 일반적으로, 재귀적 알고리즘은 반복(Iterative) 알고리즘보다 더 많은 메모리와 연산을 필요로 한다.
- 입력 값 n이 커질수록 재귀 알고리즘은 반복에 비해 비효율적일 수 있다.

- ```python
  # 반복에 비해 비효율적일 수 있지만 아래 예제와 같이 알고리즘을 이용하여 시간복잡도를 줄일 수 있다.
  
  # 반복문을 이용한 선형시간 O(n)
  
  def Iterative_Power(x, n):
      result = 1
  
      for i in range(1, n + 1):
          result *= x
  
      return result
  
  # 분할 정복을 이용한 거듭제곱 O(LogN)
  
  def Recursive_Power(x, n):
      if n == 1: return x
      if n % 2 == 0:
          y = Recursive_Power(x, n // 2)
          return y * y
      else:
          y = Recursive_Power(x, (n - 1) //2 )
          return  y * y * x
  ```

### 반복

> - ```python
>   for i in range(k):
>   	B[i] = A[i]
>   
>   # 시간 복잡도
>   O(n)
>   ```

### 재귀

> - ```python
>   f(i, k)   # i : 복사할 자리, k : 크기
>   if i = k:
>   	print(B)
>   else:
>   	B[i] = A[i]
>   	f(i+1, k)
>   
>   # 시간 복잡도
>   T(n) = T(n-1) + 1
>   -> O(n)
>   ```
>
> - 짧은 함수는 함수 처리하는 시간보다 호출/복귀 시간이 더 걸릴 수 있다.
>
>   - → 똑같은 O(n)이더라도 재귀가 더 걸릴 수 있다.

<br>

## 완전 검색

- 모든 경우의 수를 생성하고 테스트하기 때문에 수행 속도는 느리지만, 해답을 찾아내지 못할 확률이 작다.
- 이를 기반으로 그리디 기법이나 동적 계획법을 이용해서 효율적인 알고리즘을 찾을 수 있다.
- 많은 종류의 문제들이 특정 조건을 만족하는 경우나 요소를 찾는 것이다.
- 전형적으로 순열(permutation), 조합(combination), 부분집합(subsets)과 같은 조합적 문제들과 연관된다. ( 조합적 문제에 대한 brute-force 방법)

### Brute-force(고지식한 방법)

> - 문제를 해결하기 위한 간단하고 쉬운 접근법
> - 대부분의 문제에 적용 가능
> - 상대적으로 빠른 시간에 문제해결(알고리즘 설계)을 할 수 있다.
> - 문제에 포함된 자료(요소, 인스턴스)의 크기가 작다면 유용하다.
> - 알고리즘의 효율성을 판단하기 위한 척도로 사용

<br>

## 순열

### 순열 계산

> - ```python
>   N = 4
>   cnt = 0
>   
>   def perm(idx):
>       global cnt
>       if idx == N:
>           cnt += 1
>       else:
>           for i in range(N - idx):
>               perm(idx + 1)
>   
>   perm(0)
>   print(cnt)
>   ```

### for

> - ```python
>   P = ['A', 'B', 'C', 'D']
>   N = len(P)
>   ans = [0] * N
>   
>   for i in range(N):
>       ans[0] = (P[i])
>   
>       for j in range(N):
>           if j == i: continue
>           ans[1] = (P[j])
>   
>           for k in range(N):
>               if k == i or k == j: continue
>               ans[2] = (P[k])
>   
>               for l in range(N):
>                   if l == i or l == j or l == k: continue
>                   ans[3] = (P[l])
>   
>                   print(*ans)
>   
>       #             ans[3] =''
>       #         ans[2] = ''
>       #     ans[1] = ''
>       # ans[0] = ''
>   ```

### 재귀 - for

> - ```python
>   P = ['A', 'B', 'C', 'D']
>   N = len(P)
>   ans = [0] * N
>   check = [0] * N
>   
>   def perm(idx):
>       if idx == N:
>           print(*ans)
>       else:
>           for i in range(N):
>               if check[i] == 0:   # 해당 원소가 사용됐는지 체크
>                   ans[idx] = P[i]
>                   check[i] = 1    # 사용했는지 표시
>                   perm(idx+1)
>                   check[i] = 0    # 다음 반복을 위한 원상복구
>   
>   perm(0)
>   ```

### 재귀 - 비트

> - ```python
>   P = ['A', 'B', 'C']
>   N = len(P)
>   ans = [0] * N
>   
>   def perm(idx, check):
>       if idx == N:
>           print(*ans)
>   
>       for j in range(N):
>           if check & (1 << j): continue
>   
>           ans[idx] = P[j]
>           perm(idx+1, check | (1 << j))
>   
>   perm(0, 0)
>   ```

### 재귀 - 스왑

> - ```python
>   P = ['A', 'B', 'C']
>   N = len(P)
>   
>   def perm(idx):
>       if idx == N:
>           print(*P)
>       else:
>           for i in range(idx, N):
>               P[idx], P[i] = P[i], P[idx]
>               perm(idx + 1)
>               P[idx], P[i] = P[i], P[idx]
>   
>   perm(0)
>   ```

<br>

## 부분집합

### for

> - ```python
>   arr = ['A', 'B', 'C']
>   N = len(arr)
>   bits = [0] * N
>   
>   for i in range(2):
>       bits[0] = i
>   
>       for j in range(2):
>           bits[1] = j
>   
>           for k in range(2):
>               bits[2] = k
>   
>               print(bits)
>   ```

### 재귀 - for

> - ```python
>   arr = ['A', 'B', 'C']
>   N = len(arr)
>   bits = [0] * N
>   
>   def powerset(idx):
>       if idx == N:
>           print(bits, ":", end=' ')
>           for j in range(N):
>               if bits[j] == 1:
>                   print(arr[j], end=' ')
>           print()
>       else:
>           for i in range(2):
>               bits[idx] = i
>               powerset(idx+1)
>   
>   powerset(0)
>   ```
>
> - ```python
>   arr = ['A', 'B', 'C']
>   N = len(arr)
>   bits = [0] * N
>   
>   def powerset(idx):
>       if idx == N:
>           print(bits, ":", end=' ')
>           for j in range(N):
>               if bits[j] == 1:
>                   print(arr[j], end=' ')
>           print()
>           return 
>   
>       bits[idx] = 1
>       powerset(idx + 1)
>       bits[idx] = 0
>       powerset(idx + 1)
>   
>   powerset(0)
>   ```

<br>

## 조합

### for

> - 특정 상황에서만 가능하다 !
>
>   - 골라야하는 개수가 변하지 않는 경우는 for문으로 가능 (nCr에서 r이 변하는 상황이 아닌 경우 !)
>   - But tc마다 구해야되는 개수가 변할 수 있으면 불가능
>
> - ```python
>   arr = ['A', 'B', 'C', 'D', 'E']
>   n = len(arr)
>   r = 3
>   
>   for i in range(0, n-3 +1):
>       for j in range(i+1, n-2 +1):
>           for k in range(j+1, n-1 +1):
>               print('인덱스:', i, j, k)
>   ```

### 재귀

> - ```python
>   arr = ['A', 'B', 'C', 'D', 'E']
>   n = len(arr)
>   r = 3
>   
>   def combination(i, j, n, r):    # i : C에 사용되는 인덱스, j : 선택구간의 시작
>       if i == r:
>           print(ans)
>       else:
>           for k in range(j, n-r +i + 1):
>               ans[i] = arr[k]
>               combination(i+1, k+1, n, r)
>   
>   ans = [0] * r
>   combination(0, 0, n, r)
>   ```

<br>

#  `04.15` ~ `04.16` 

## 참고

- 최적화나 결정 문제 → 완전 검색 → 경우의 수가 많아짐 → 시간이 오래걸림
  - 현실의 대부분 문제는 최적화 문제
- 깊이우선탐색이 최적화 문제를 풀 때 너비우선탐색보다 성능이 좋다,
  - 후보 해를 빨리 만나기 때문에 가지치게 활용 가능 !
  - 가지치기 성능이 좋아지려면 좋은 후보해(최적해에 가까운 해)를 빨리 찾아야 함
- 최고 우선 탐색(A*) : BFS + 우선순위큐
  - 최적해에 가까운 해를 파악해서 먼저 검사 ! → 최적해에 가까운 해인지 판단이 필요
  - 적용하기가 어려움, 어떻게 우선순위를 정하는지도 알아야하기 때문
- 백트래킹과 DP가 최적화에 좋음
- 완전검색과 트리를 잘 그리는 것이 중요!
- 문제를 풀 때 처음 방향 설정이 중요 !
- 접근을 못하겠으면 부분 집합이나 순열, 조합의 답을 생각해보기
  - 순서가 있는 경우 순열 !
  - 구하려는 답이 몇 개를 골라야 하는 경우 → 부분집합
  - 몇 개를 구하는지 알려주는 경우 → 조합, 부분집합, 이때는 조합이 더 좋음
- 경우를 트리로 짜보기(백트래킹 때 좋음)
- 가지치기 때 활용하는 것
  - 지금까지 발견한 가장 좋은 후보 해
  - 지금까지 한 선택들(많이 쌓여있을수록 결정하기 쉬움)

<br>

# `04.19`

## 분할 정복

- 분할(Divide) : 해결할 문제를 여러 개의 작은 부분으로 나눈다.
- 정복(Conquer) : 나눈 작은 문제를 각각 해결한다.
- 통합(Combine) : 필요하다면 해결된 해답을 모은다.
- ex) 거듭제곱
  - 반복(Iterative)알고리즘 : O(n)
  - 분할 정복 기반의 알고리즘 : O(logn)
- 정렬 방법 중에서 평균적으로 병합 정렬과 퀵 정렬이 좋다.
- 분할 정복을 할 때에는 메모이제이션을 안함
  - 일반적으로 문제를 한번씩으로만 풀기 때문에
  - 동적 계획법은 중복된 계산 작업을 하기 때문에 메모이제이션을 함

### 병합 정렬(Merge Sort)

> - 여러 개의 정렬된 자료의 집합을 병합하여 한 개의 정렬된 집합으로 만드는 방식
> - 자료를 최소 단위의 문제까지 나눈 후 차례대로 정렬하여 최종 결과를 얻어냄
> - top-down 방식
> - 대략적으로 각 높이에 대해서 n번 연산 x 높이는 logn
> - 시간 복잡도 : O(nlogn) → 항상 보장
>   - 퀵 정렬은 입력데이터에 따라 달라지고 최악의 경우 O(n^2)이 나올 수 있다.
> - 메모리를 많이 사용하는 문제가 있다. → 인덱스로 해결하는 방법이 있다.(서로의 영역에는 관여하지 않으므로)

### 퀵 정렬

> - 주어진 배열을 두 개로 분할하고, 각각을 정렬
> - 병합 정렬과 다른 점
>   - 병합 정렬은 그냥 두 부분으로 나누는 반면에, 퀵 정렬은 분할할 때, 기준 아이템(pivot item) 중심으로, 이보다 작은 것은 왼편, 큰 것은 오른편에 위치시킨다.
>   - 각 부분 정렬이 끝난 후 병합 정렬은 '병합'이라는 후처리가 필요하지만 퀵 정렬은 필요로 하지 않는다.
> - 피봇값들보다 큰 값은 오른쪽, 작은 값들은 왼쪽 집합에 위치시키고 피봇을 두 집합의 가운데에 위치시킨다.
> - 처음, 마지막, 중간 값을 피봇으로 많이 사용
> - Hoare-Partition : 제일 왼쪽이 피봇
> - Lomuto-Partition : 제일 오른쪽이 피봇
> - 고정된 위치가 아니라 랜덤함수를 이용하여 랜덤하게 찍으면 평균적으로 시간이 좋음
> - 최악의 경우 O(n^2)

### 이진 검색(Binary Search)

> - 자료의 가운데에 있는 항목의 키 값과 비교하여 다음 검색의 위치를 결정하고 검색을 계속 진행하는 방법
>   - 목적 키를 찾을 때까지 이진 검색을 순환적으로 반복 수행함으로써 검색 범위를 반으로 줄여가면서 보다 빠르게 검색을 수행함
> - 이진 검색을 하기 위해서는 자료가 정렬된 상태여야한다.
> - 재귀보다 반복 구조가 더 빠르다.

### 분할 정복의 활용

> - 병합 정렬은 외부 정렬의 기본이 되는 정렬 알고리즘
> - 멀티코어(Multi-Core) CPU나 다수의 프로세서에서 정렬 알고리즘을 병렬화하기 위해 병합 정렬 알고리즘이 활용
> - 퀵 정렬은 매우 큰 입력 데이터에 대해서 좋은 성능을 보임(주어진 값에 따라서 효율이 달라짐)
>   - ex) 특정 유전자를 효율적으로 찾는대 사용

<br>

## 백트래킹(Backtracking)

- 여러가지 선택지들이 존재하는 상황에서 한가지를 선택
- 선택이 이루어지면 새로운 선택지들의 집합이 생성
- 이런 선택을 반복하면서 최종 상태에 도달

### 백트래킹과 깊이 우선 탐색과의 차이

> - 어떤 노드에서 출발하는 경로가 해결책으로 이어질 것 같지 않으면 더 이상 그 경로를 따라가지 않음으로써 시도의 횟수를 줄임(Prunning 가지치기)
> - 깊이 우선 탐색이 모든 경로를 추척하는데 비해 백트래킹은 불필요한 경로를 조기에 차단
> - N! 가지의 경우의 수를 가진 문제에 대해 깊이 우선 탐색을 가하면 당연히 처리 불가능한 문제, ex. 12! → 매우 큼
> - 백트래킹 알고리즘을 적용하면 일반적으로 경우의 수가 줄어들지만 이 역시 최악의 경우에는 여전히 지수함수 시간(Exponential Time)을 요하므로 처리 불가능

<br>

## 트리

- 트리는 싸이클이 없는 무향 연결 그래프이다.
  - 두 노드(or 정점) 사이에는 유일한 경로가 존재한다.
  - 각 노드는 최대 하나의 부모 노드가 존재할 수 있다.
  - 각 노드는 자식 노드가 없거나 하나 이상이 존재할 수 있다.
- 비선형 구조
  - 원소들 간에 1:n 관계를 가지는 자료구조
  - 원소들 간에 계층관계를 가지는 계층형 자료구조
- 한 개 이상의 노드로 이루어진 유한 집합
  - 노드 중 부모가 없는 노드를 루트(root)라 한다.
  - 나머지 노드들은 분리집합으로 분리될 수 있고 루트의 서브트리(subtree)라고 한다.

### 이진 트리(Binary Tree)

> - 모든 노드들이 최대 2개의 서브 트리를 갖는 특별한 형태의 트리
> - 각 노드가 자식 노드를 최대한 2개까지만 가질 수 있는 트리
>   - 왼쪽, 오른쪽 자식

### 포화 이진 트리(Full Binary Tree)

> - 모든 레벨에 노드가 포화상태로 채워져있는 이진 트리
> - 루트를 1번으로 하여 2^(h+1) -1까지 정해진 위치에 대한 노드 번호를 가짐

### 완전 이진 트리(Complete Binary Tree)

> - 높이가 h이고 노드 수가 n개일 때(단, 2^h ≤ n < 2(h+1) -1) 포화 이진 트리의 노드 번호 1번부터 n번까지 빈 자리가 없는 이진 트리

### 편향 이진 트리(Skewed Binary Tree)

> - 높이 h에 대한 최소 노드 개수의 노드를 가지면서 한쪽 방향의 자식 노드만을 가진 이진 트리

### 이진 트리 순회

> - 순회(traversal) : 트리의 노드들을 체계적으로 방문하는 것
> - 3가지의 기본적인 순회방법
>   - 전위 순회(preorder traversal) : VLR
>   - 중위 순회(inorder traversal) : LVR
>   - 후위 순회 (postorder traversal) : LRV
> - 포화나 완전 이진 트리에서는 자식 노드 번호를 연산으로 결정
>   - 왼쪽자식 : n * 2
>   - 오른쪽 자식 : n * 2 + 1

### 이진 탐색 트리

> - 탐색작업을 효율적으로 하기 위한 자료구조
> - 모든 원소는 서로 다른 유일한 키를 갖는다.
> - Key(왼쪽 서브 트리) < Key(루트 노드) < Key(오른쪽 서브 트리)
> - 왼쪽 서브 트리와 오른쪽 서브 트리도 이진 탐색 트리다.
> - 중위 순회하면 오름차순으로 정렬된 값을 얻을 수 있다.
> - 탐색 연산
>   - 루트에서 탐색 시작
>   - 탐색할 키 값 x를 루트 노드의 키 값 k와 비교
>     - 같으면 탐색 성공, 작으면 왼쪽, 크면 오른쪽
>   - 순환적으로 탐색 연산 반복

### 힙

> - 완전 이진 트리에 있는 노드 중에서 키 값이 가장 큰 노드나 키 값이 가장 작은 노드를 찾기 위해서 만든 자료구조
> - 최대 힙(max heap)
>   - 완전 이진 트리
>   - 부모 > 자식
> - 최소 힙(min heap)
>   - 완전 이진 트리
>   - 부모 < 자식부모 < 자식
> - 힙 연산 - 삽입
>   - 삽입할 자리 확장 → 확장한 자리에 삽입할 원소 저장 → 부모 노드와 비교
> - 힙 연산 - 삭제
>   - 힙에서는 루트 노드의 원소만을 삭제할 수 있다.
>   - 루트 노드의 원소를 삭제하여 반환
>   - 힙의 종류에 따라 최대값 또는 최소값을 구할 수 있다.
> - 활용하는 대표적인 2가지 : 특별한 큐의 구현과 정렬
> - 우선순위 큐를 구현하는 가장 효율적인 방법은 힙을 사용하는 것
>   - 노드 하나의 추가/삭제가 시간 복잡도가 O(logN)이고 최대값/쵯소값을 O(1)에 구할 수 있다.
>   - 완전 정렬보다 관리 비용이 적다.
> - 정렬을 위한 2단계
>   - 하나의 값을 힙에 삽입(반복)
>   - 힙에서 순차적(오름차순)으로 값을 하나씩 제거
>
> #### 힙 정렬
>
> > - 이진트리와 유사한 방법으로 수행한다.
> > - 배열에 저장된 자료를 정렬하기에 유용하다.
> > - 힙 정렬의 시간 복잡도
> >   - N개의 노드 삽입 연산 + N개의 노드 삭제 연산
> >   - 삽입과 삭제 연산은 각각 O(logN)이다.
> >   - 따라서, 전체 정렬은 O(NlogN)이다.